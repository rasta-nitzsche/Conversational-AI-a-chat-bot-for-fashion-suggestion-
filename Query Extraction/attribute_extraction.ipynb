{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193d016d",
   "metadata": {},
   "source": [
    "## Requirments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656460f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rasta\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a913da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\rasta\\anaconda3\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd9298a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.19.5)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.62.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (58.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rasta\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.3.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8a29d",
   "metadata": {},
   "source": [
    "### Call the fashion classifier model (We pushed it on HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a9b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model_id = \"rasta/distilbert-base-uncased-finetuned-fashion\"\n",
    "classifier = pipeline(\"text-classification\", model=model_id)\n",
    "\n",
    "def classify(text):\n",
    "    preds = classifier(text, return_all_scores=True)\n",
    "    if preds[0][0]['score']  <= preds[0][1]['score']:\n",
    "        return \"Not Fashion\"\n",
    "    else:\n",
    "        return \"Fashion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d11d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4bea3",
   "metadata": {},
   "source": [
    "## Attribute extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c32bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_extraction(txt):          # Method: we get each noun and its descriptions (by going back until no more) and filter the fashion from the non fashion\n",
    "    tokenized = sent_tokenize(txt)\n",
    "\n",
    "    attributes = []\n",
    "    for i in tokenized:                                 # Get the POS tagging with NLTK\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "\n",
    "    for i,w in enumerate(tagged) :\n",
    "        if w[1] in ['NN','NNS','RB'] :                # All the nouns are considered as attributes \n",
    "            ind =i \n",
    "            attr = w[0]\n",
    "            while tagged[ind-1][1] in ['JJ','VBN','NN','RB','VBD','EX']:                # All these tags are considered as descriptions of the attributes\n",
    "                    attr = tagged[ind-1][0] + ' ' +  attr\n",
    "                    ind = ind - 1\n",
    "                    \n",
    "            if len(attr.split())==1 and txt.split()[0].lower()=='will':         # To avoid the classification of will as a noun\n",
    "                attr = tagged[ind-1][0] + ' ' +  attr\n",
    "                \n",
    "            if classify(attr) == 'Fashion':\n",
    "                attributes.append(attr)\n",
    "            for a in attributes:\n",
    "                for b in attributes:\n",
    "                    if (a!=b) and (a in b):\n",
    "                        attributes.remove(a)\n",
    "                        \n",
    "            # The following is to avoid the the classification of fit and match as nouns\n",
    "            for a in attributes:\n",
    "                if 'fit' in a :\n",
    "                    attributes = list(map(lambda x: x.replace(a, a.replace(' fit','')), attributes))\n",
    "                if 'match' in a :  \n",
    "                    attributes = list(map(lambda x: x.replace(a, a.replace(' match','')), attributes))                                       \n",
    "                \n",
    "    return attributes        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "565604ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red top', 'green boots']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_extraction('Should I wear red top with green boots?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27992cbd",
   "metadata": {},
   "source": [
    "### Simple module of the extraction pipeline : isQuestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71451239",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "\n",
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features\n",
    "\n",
    "featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]\n",
    "\n",
    "# 10% of the total data\n",
    "size = int(len(featuresets) * 0.1)\n",
    "\n",
    "# first 10% for test_set to check the accuracy, and rest 90% after the first 10% for training\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "\n",
    "# get the classifer from the training set\n",
    "classifiers = nltk.NaiveBayesClassifier.train(train_set)\n",
    "# to check the accuracy - 0.67\n",
    "# print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "question_types = [\"whQuestion\",\"ynQuestion\"]\n",
    "def is_ques_using_nltk(ques):\n",
    "    question_type = classifiers.classify(dialogue_act_features(ques)) \n",
    "    return question_type in question_types\n",
    "\n",
    "\n",
    "question_pattern = [\"do i\", \"do you\", \"what\", \"who\", \"is it\", \"why\",\"would you\", \"how\",\"is there\",\n",
    "                    \"are there\", \"is it so\", \"is this true\" ,\"to know\", \"is that true\", \"are we\", \"am i\", \n",
    "                   \"question is\", \"tell me more\", \"can i\", \"can we\", \"tell me\", \"can you explain\",\n",
    "                   \"question\",\"answer\", \"questions\", \"answers\", \"ask\"]\n",
    "\n",
    "helping_verbs = [\"is\",\"am\",\"can\", \"are\", \"do\", \"does\"]\n",
    "# check with custom pipeline if still this is a question mark it as a question\n",
    "\n",
    "def is_question(question):\n",
    "    question = question.lower().strip()\n",
    "    if not is_ques_using_nltk(question):\n",
    "        is_ques = False\n",
    "        # check if any of pattern exist in sentence\n",
    "        for pattern in question_pattern:\n",
    "            is_ques  = pattern in question\n",
    "            if is_ques:\n",
    "                break\n",
    "\n",
    "        # there could be multiple sentences so divide the sentence\n",
    "        sentence_arr = question.split(\".\")\n",
    "        for sentence in sentence_arr:\n",
    "            if len(sentence.strip()):\n",
    "                # if question ends with ? or start with any helping verb\n",
    "                # word_tokenize will strip by default\n",
    "                first_word = nltk.word_tokenize(sentence)[0]\n",
    "                if sentence.endswith(\"?\") or first_word in helping_verbs:\n",
    "                    is_ques = True\n",
    "                    break\n",
    "        return is_ques    \n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc3593",
   "metadata": {},
   "source": [
    "## Semantic check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629256a7",
   "metadata": {},
   "source": [
    "Semantic check is composed of two modules: a hard coded method and an automated model. In order to optimize the response time, we would like to avoid using the model for each query. With the hard coded method we can verify 90% of usual query (with match, suit, ...). Also, the semantic check has a 100% accuracy on detecting semantically true sentences. So here is the proposed pipeline : Fist we check with the hard coded method, if it is true then we return. If not, we go through the automated model in order to clear all the doubts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7f561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "model_semantick_id = \"PriaPillai/distilbert-base-uncased-finetuned-query\"\n",
    "classifier_sem = pipeline(\"text-classification\", model=model_semantick_id)\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "verb_pattern = [ps.stem(i) for i in ['match', 'suit', 'fit', 'wear', 'pair']]\n",
    "# 'be', 'go', 'are'\n",
    "\n",
    "def semantic_check_hard_coded(txt):\n",
    "    tokenized = sent_tokenize(txt)\n",
    "    verbs = []\n",
    "    \n",
    "    for i in tokenized:\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "\n",
    "    for i,w in enumerate(tagged) :\n",
    "        if w[1] in ['VB','VBD','VBN','VBG','VBP','VBZ'] :\n",
    "            verbs.append(ps.stem(w[0]))\n",
    "    \n",
    "    for v in verbs:\n",
    "        if v in verb_pattern :\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def semantic_check(text):\n",
    "    if semantic_check_hard_coded(text):\n",
    "        return True\n",
    "    preds = classifier_sem(text, return_all_scores=True)\n",
    "    if preds[0][0]['score']  <= preds[0][1]['score']:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f721b64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check(\"Can I eat blue jeans ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "567816fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check(\"What matches my white shirt ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a0827996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check(\"What can match my white shirt ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7877e43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check('What can i wear with green pants?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c730bb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check(\"Will blue jeans and a black shirt match ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fec7840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check('Do you want a blue necklace ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a209254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check(\"What can suit my blue jeans and dark shirt ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b9e1318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check(\"Can I actually have a blue pants paired with a white shirt ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71c6b4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check(\"Can blue pants be a good fit with white shirt ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e229d9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check(\"Hey Aleksey, do you think it's a good work ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "889edb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check(\"Maybe I should ask the model again... What can I wear with my skinny blue jeans ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "117848d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_check(\"I want to eat a blue pant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc8af7",
   "metadata": {},
   "source": [
    "## Whole Pipeline function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10cc8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction_pipeline(query):\n",
    "    if not is_question(query):\n",
    "        print(\"I am not understanding you, please enter a question that is related to fashion\")\n",
    "        return []\n",
    "    elif not semantic_check(query) :\n",
    "        print(\"I am not sure to get your query can you please try again ?\")\n",
    "        return []\n",
    "    else:\n",
    "        return attribute_extraction(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12422336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue jean', 'black shirt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_pipeline(\"Will a blue jean and a black shirt fit ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
